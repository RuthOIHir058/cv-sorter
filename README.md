# CV Sorter: AI-Powered Resume Ranking System

## ğŸ“Œ Project Overview
This project implements an AI-based Applicant Tracking System (ATS) that parses resumes, extracts skills and experiences, and ranks candidates against job descriptions using embeddings and semantic similarity.  
Developed as part of a capstone project.

---

## ğŸš€ Features
- ğŸ“„ **Resume Parsing**: Extracts text from `.pdf`, `.docx`, `.txt` resumes.
- ğŸ“ **Job Description Parsing**: Extracts skills, responsibilities, and requirements.
- ğŸ¤– **Candidate Ranking**:
  - Skill overlap (Jaccard similarity)
  - Sentence-transformer embeddings (`all-MiniLM-L6-v2`)
  - Configurable scoring weights
- ğŸ“Š **Evaluation Metrics**:
  - Precision@k, MAP, Kendall Tau
- ğŸŒ **Streamlit App**:
  - Upload resumes & JDs
  - Get ranked candidates with scores
  - Export results as CSV/JSON
- ğŸ³ **Dockerized Deployment**

---

## ğŸ—‚ï¸ Project Structure
cv-sorter/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ resumes/ # Raw resumes
â”‚ â”œâ”€â”€ parsed_resumes/ # Parsed JSON resumes
â”‚ â””â”€â”€ sample_jd.txt # Example job description
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ app_streamlit.py # Streamlit UI
â”‚ â”œâ”€â”€ parse_resumes.py # Resume parsing
â”‚ â”œâ”€â”€ parse_jd.py # JD parsing
â”‚ â”œâ”€â”€ embeddings.py # Embeddings
â”‚ â”œâ”€â”€ scorer.py # Scoring functions
â”‚ â”œâ”€â”€ ranker.py # Ranking pipeline
â”‚ â”œâ”€â”€ eval_metrics.py # Evaluation metrics
â”‚ â””â”€â”€ utils.py # Helper functions
â”œâ”€â”€ tests/
â”‚ â””â”€â”€ gold_standard.json # Human-annotated test set
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â””â”€â”€ report.pdf (to be added)

---

## âš™ï¸ Installation & Setup

1. Clone the repository
```bash
git clone https://github.com/RuthOIHir058/cv-sorter.git
cd cv-sorter
2. Create virtual environment
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
3. Run locally
streamlit run src/app_streamlit.py
App will run at: http://localhost:8501

ğŸ³ Docker Deployment
docker build -t cv-sorter:latest .
docker run -p 8501:8501 cv-sorter:latest
ğŸ“Š Evaluation
Run evaluation against gold standard:

python -m src.eval_metrics
Metrics include:

Precision@k

Average Precision

Mean Average Precision (MAP)

Kendall Tau ranking correlation

ğŸ“‘ Report & Slides
report.pdf â€“ Contains goals, design rationale, metrics, and analysis.

slides.pdf â€“ Summarizes technical approach and results.

demo â€“ Streamlit app demonstration.

ğŸ“¬ Contact
Author: Sirsha Dattasarma
Email: sirshodattasarma@gmail.com
GitHub: RuthOIHir058
